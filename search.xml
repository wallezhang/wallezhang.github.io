<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何使用GitLab的CI/CD进行持续集成和部署]]></title>
    <url>%2F2017%2F08%2Fhow-to-use-gitlab-ci%2F</url>
    <content type="text"><![CDATA[Gitlab CI/CD与Docker的简单集成Gitlab是使用Ruby On Rails编写的GIT版本控制软件，原生提供了CI/CD的支持。接下来我们看看如何使用CI/CD将项目部署在Docker上。 什么是CI和CDCI（Continuous Integration）是一种软件开发的实践，开发人员每次将代码PUSH到仓库中，都会进行一次构建和测试，这样的流程每天会发生很多次。 CD（Continuous Delivery）是一个软件工程方法论，描述了在软件开发的周期中，尽可能的不需要人工参与的进行持续集成、自动测试和自动部署的能力。 开始集成只需要简单的两步即可让项目支持CI/CD： 在项目根目录中增加.gitlab-ci.yml文件 配置一个runner 创建一个.gitlab-ci.yml文件什么是.gitlab-ci.yml在这个文件中，配置了CI/CD过程中都需要做哪些事情，每次PUSH代码到仓库中，GITLAB会首先检索这个文件，然后根据配置选择一个runner，去执行配置的脚本。 创建一个简单的.gitlab-ci.yml文件 .gitlab-ci.yml是一个YAML格式的文件 下面是一个简单的示例 12345678910111213141516171819202122232425262728293031323334353637image: docker:latestbefore_script: - echo "Before script section" - echo "For example you might run an update here or install a build dependency" - echo "Or perhaps you might print out some debugging details" after_script: - echo "After script section" - echo "For example you might do some cleanup here" build1: stage: build only: - branches script: - echo "build1 is done." tags: - tag1 - tag2test1: stage: test script: - echo "Do a test here" - echo "For example run a test suite" tags: - tag1 - tag2 deploy1: stage: deploy script: - echo "Do your deploy here" tags: - tag1 - tag2 将.gitlab-ci.ymlPUSH到仓库中123git add .gitlab-ci.ymlgit commit -m &quot;Add .gitlab-ci.yml&quot;git push origin master 配置一个runner本文简单的使用一个docker container来作为runner，安装方式可以参考官方指南 共享？or专用？runner分为共享的和专用的，共享的runner可以被所有项目使用，如果项目的配置是允许使用共享的runner，专用的runner只能被指定的项目使用。创建不同的runner是根据token来指定的 创建专用runner：项目Master可以在项目的Settings -&gt; CI/CD Pipelines界面中找到Specific Runners一栏，找到该项目对应的Token和CI的URL 创建共享runner：管理员可以在Admin -&gt; Runners界面中看到Token 然后在gitlab-runner的容器中运行以下命令进行注册：sudo gitlab-ci-multi-runner register runner的executor这里套用官方的一个表格 Executor Shell Docker Docker-SSH VirtualBox Parallels SSH Kubernetes 每次构建都使用新环境 no ✓ ✓ ✓ ✓ no ✓ 迁移runner no ✓ ✓ 部分 部分 no ✓ 0配置支持并发构建 no (1) ✓ ✓ ✓ ✓ no ✓ 复杂的构建环境 no (2) ✓ ✓ ✓ (3) ✓ (3) no ✓ 调试构建信息 容易 中等 中等 困难 困难 容易 中等 可以支持，但是在大多数场景下，如果构建中使用runner所在机器上安装的服务，则会出现问题。 需要手工安装所有依赖 需要安装额外的软件，如使用Vagrant 本文使用的executor是Docker .gitlab-ci.yml主要参数详解before_script在每个Job执行之前都会执行before_script中定义的命令 after_script在每个Job执行之后都会执行after_script中定义的命令 stages定义Job所使用的阶段，如果不显示的指定，默认的阶段是：build、test、deploy，按照顺序依次执行。 每个阶段中的Job是并行执行的，当一个阶段的所有JOB都成功执行之后，会顺序的执行下一个阶段中定义的Job。 如果Job不指定stage参数，默认属于test阶段。 Jobs允许定义无限多个Job，每一个Job必须有一个唯一的名字，不能用关键字参数作为名字。 Job的参数列表 参数 是否必须 说明 script 是 定义一个用来执行的shell脚本 image 否 使用的docker image services 否 使用的docker service stage 否 定义该Job的所属阶段（默认是：test） type 否 stage的别名 variables 否 在Job范围内定义变量 only 否 定义一个用来创建该Job的git refs列表 except 否 定义一个不会创建该Job的git refs列表 tags 否 定义一个用来选择runner的标签列表 allow_failure 否 允许该Job失败，失败的Job不会在提交状态上体现出来 when 否 定义何时会执行该Job，取值范围：on_success，on_failure，always或manual dependencies 否 定义该Job依赖的其它Job，可以在这些Job之间传递artifacts artifacts 否 定义Job的artifact列表 cache 否 定义在接下来的Job中应该被缓存的文件列表 before_script 否 覆盖全局定义的before_script命令 after_script 否 覆盖全局定义的after_script命令 environment 否 定义当前Job用来完成部署的环境变量名称 coverage 否 定义该Job的代码覆盖设置 script一个被runner执行的shell脚本，可以是单个命令或者命令数组 only 和 exceptonly和except是两个用来定义何时需要创建Job的参数： only定义一组用来创建Job的分支名称或者标签名称 except定义一组不会创建Job的分支名称或者标签名称 有一些规则来定义refs： only和except可以同时使用 only和except允许使用正则表达式 only和except允许使用指定的关键字：branches，tags和triggers only和except允许指定一个远程仓库的地址用来过滤fork的Job 下面这个示例，Job只会在以issue-开头的refs有提交的时候运行 1234567job: # use regexp only: - /^issue-.*$/ # use special keyword except: - branches 下面这个示例，Job只会在被创建tag的分支上运行，或者被API请求触发 12345job: # use special keywords only: - tags - triggers tagstags参数用来选择runner，必须全部匹配一个runner的tag。以数组的形式表示。 allow_failure表示允许该Job失败不影响阶段最终的运行的结果。 查看项目的CI/CD结果在项目的Pipelines界面可以看到每一次构建的执行过程和执行结果。]]></content>
      <categories>
        <category>代码管理</category>
      </categories>
      <tags>
        <tag>GitLab</tag>
        <tag>CI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker部署MongoDB集群]]></title>
    <url>%2F2017%2F08%2Fhow-to-deploy-mongodb-cluster-with-docker%2F</url>
    <content type="text"><![CDATA[使用Docker部署MongoDB Cluster环境准备 四台服务器，分别命名为ServerA、ServerB、ServerC、ServerD 2 Shard（1 Primary 1 Secondary 1 Arbiter） Nodes 3 Config Nodes 4 Router Nodes Docker版本：1.12.5 Docker Compose版本：1.9.0 Docker镜像MongoDB官方镜像 docker-compose.ymlServerA配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: '2'services: configsrv: image: mongo command: mongod --keyFile /data/configdb/mongodb-keyfile --oplogSize 1024 --replSet configrs --port 27017 --configsvr --wiredTigerCacheSizeGB 5 volumes: - /data/configsrv_db:/data/configdb ports: - "27018:27017" restart: always container_name: configsrv ulimits: nofile: soft: 300000 hard: 300000 rs1_node: image: mongo command: mongod --keyFile /data/db/mongodb-keyfile --oplogSize 10240 --replSet rs1 --directoryperdb --port 27017 --shardsvr volumes: - /data/rs1_node_db:/data/db ports: - "27019:27017" restart: always container_name: rs1_node ulimits: nofile: soft: 300000 hard: 300000 router: image: mongo command: mongos --keyFile /data/db/mongodb-keyfile --configdb configrs/ServerA:27018,ServerB:27018,ServerC:27018 ports: - "27017:27017" volumes: - /data/router_db:/data/db restart: always container_name: router ulimits: nofile: soft: 300000 hard: 300000 ServerB配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162version: '2'services: configsrv: image: mongo command: mongod --keyFile /data/configdb/mongodb-keyfile --oplogSize 1024 --replSet configrs --port 27017 --configsvr --wiredTigerCacheSizeGB 5 volumes: - /data/configsrv_db:/data/configdb ports: - "27018:27017" restart: always container_name: configsrv ulimits: nofile: soft: 300000 hard: 300000 rs1_node: image: mongo command: mongod --keyFile /data/db/mongodb-keyfile --oplogSize 10240 --replSet rs1 --directoryperdb --port 27017 --shardsvr volumes: - /data/rs1_node_db:/data/db ports: - "27019:27017" restart: always container_name: rs1_node ulimits: nofile: soft: 300000 hard: 300000 rs2_arbiter: image: mongo command: mongod --keyFile /data/db/mongodb-keyfile --oplogSize 1024 --replSet rs2 --directoryperdb --port 27017 --shardsvr --wiredTigerCacheSizeGB 1 --nojournal --smallfiles volumes: - /data/rs2_arbiter_db:/data/db ports: - "27020:27017" restart: always container_name: rs2_arbiter ulimits: nofile: soft: 300000 hard: 300000 router: image: mongo command: mongos --keyFile /data/db/mongodb-keyfile --configdb configrs/ServerA:27018,ServerB:27018,ServerC:27018 ports: - "27017:27017" volumes: - /data/router_db:/data/db restart: always container_name: router ulimits: nofile: soft: 300000 hard: 300000 ServerC配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162version: '2'services: configsrv: image: mongo command: mongod --keyFile /data/configdb/mongodb-keyfile --oplogSize 1024 --replSet configrs --port 27017 --configsvr --wiredTigerCacheSizeGB 5 volumes: - /data/configsrv_db:/data/configdb ports: - "27018:27017" restart: always container_name: configsrv ulimits: nofile: soft: 300000 hard: 300000 rs2_node: image: mongo command: mongod --keyFile /data/db/mongodb-keyfile --oplogSize 10240 --replSet rs2 --directoryperdb --port 27017 --shardsvr volumes: - /data/rs2_node_db:/data/db ports: - "27019:27017" restart: always container_name: rs2_node ulimits: nofile: soft: 300000 hard: 300000 rs1_arbiter: image: mongo command: mongod --keyFile /data/db/mongodb-keyfile --oplogSize 1024 --replSet rs1 --directoryperdb --port 27017 --shardsvr --wiredTigerCacheSizeGB 1 --nojournal --smallfiles volumes: - /data/rs1_arbiter_db:/data/db ports: - "27020:27017" restart: always container_name: rs1_arbiter ulimits: nofile: soft: 300000 hard: 300000 router: image: mongo command: mongos --keyFile /data/db/mongodb-keyfile --configdb configrs/ServerA:27018,ServerB:27018,ServerC:27018 ports: - "27017:27017" volumes: - /data/router_db:/data/db restart: always container_name: router ulimits: nofile: soft: 300000 hard: 300000 ServerD配置文件 1234567891011121314151617181920212223242526272829303132version: '2'services: rs2_node: image: mongo command: mongod --keyFile /data/db/mongodb-keyfile --oplogSize 10240 --replSet rs2 --directoryperdb --port 27017 --shardsvr volumes: - /data/rs2_node_db:/data/db ports: - "27019:27017" restart: always container_name: rs2_node ulimits: nofile: soft: 300000 hard: 300000 router: image: mongo command: mongos --keyFile /data/db/mongodb-keyfile --configdb configrs/ServerA:27018,ServerB:27018,ServerC:27018 ports: - "27017:27017" volumes: - /data/router_db:/data/db restart: always container_name: router ulimits: nofile: soft: 300000 hard: 300000 启动前准备工作 创建mongodb-keyfile文件 12openssl rand -base64 741 &gt; mongodb-keyfilechmod 600 mongodb-keyfile 创建宿主机的volume文件夹 初始化Config节点重要：在初始化启动前需要去掉docker-compose.yml配置文件中的--keyFile参数 启动节点在ServerA、ServerB和ServerC三台服务器上运行命令：docker-compose up -d configsrv 初始化利用mongo连接到ServerA节点，输入以下命令创建管理用户： 12345use admindb.createUser(&#123;user: "mongoUserAdmin", pwd: "123456", roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ] &#125;)db.createUser(&#123;user: "mongoRootAdmin", pwd: "123456", roles: [ &#123; role: "root", db: "admin" &#125; ] &#125;) 初始化ReplicaSet信息 1234567891011rs.initiate( &#123; _id: "configrs", configsvr: true, members: [ &#123; _id : 0, host : "ServerA:27018" &#125;, &#123; _id : 1, host : "ServerB:27018" &#125;, &#123; _id : 2, host : "ServerC:27018" &#125; ] &#125;) 初始化Shard1节点重要：在初始化启动前需要去掉docker-compose.yml配置文件中的--keyFile参数 启动节点在ServerA和ServerB两台服务器上运行命令：docker-compose up -d rs1_node 在ServerC服务器上运行命令：docker-compose up -d rs1_arbiter 初始化利用mongo连接到ServerA节点，创建管理用户 12345use admindb.createUser(&#123;user: "mongoUserAdmin", pwd: "123456", roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ] &#125;)db.createUser(&#123;user: "mongoRootAdmin", pwd: "123456", roles: [ &#123; role: "root", db: "admin" &#125; ] &#125;) 初始化ReplicaSet信息 123456789rs.initiate( &#123; _id : "rs1", members: [ &#123; _id : 0, host : "ServerA:27019" &#125;, &#123; _id : 1, host : "ServerB:27019" &#125; ] &#125;) 增加Arbiter节点 1rs.addArb("ServerC:27020") 查看rs状态：rs.status() 初始化Shard2节点与Shard1节点雷同，只需要修改对应的服务器IP 重启Config和Shard节点取消--keyFile参数的注释，删掉上述创建的所有container 利用docker-compose再次启动上面所有节点 启动Router节点使用命令docker-compose up -d router在四台服务器上启动路由节点 配置Cluster增加Shard节点使用mongo连接到任意一台服务器的router节点，然后执行以下命令将Shard节点加入到当前Cluster中 1234use admindb.auth("&lt;username&gt;","&lt;password&gt;")sh.addShard("rs1/ServerA:27019")sh.addShard("rs2/ServerD:27019") 启动Sharding在对collection进行sharding之前一定要先对数据库启动sharding 12sh.enableSharding("&lt;database&gt;")sh.shardCollection( "&lt;database&gt;.&lt;collection&gt;", &#123; _id : "hashed" &#125; ) 参考资料 官方资料：Deploy a Sharded ClusterHashed Sharding]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
</search>
